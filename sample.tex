\documentclass{sigchi}

% Use this command to override the default ACM copyright statement (e.g. for preprints).
% Consult the conference website for the camera-ready copyright statement.
\toappear{
Permission to make digital or hard copies of all or part of this
work for personal or classroom use is granted without fee provided that
copies are not made or distributed for profit or commercial advantage and
that copies bear this notice and the full citation on the first page. To
copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior specific permission and/or a fee.\\
{\confname{CSCW'15}}, March 14-18, 2015, Vancouver, Canada.\\
%Copyright 2012 ACM 978-1-4503-1015-4/12/05...\$10.00.
}

% Arabic page numbers for submission.
% Remove this line to eliminate page numbers for the camera ready copy
\pagenumbering{arabic}


% Load basic packages
\usepackage{balance}  % to better equalize the last page
\usepackage{graphics} % for EPS, load graphicx instead
\usepackage{times}    % comment if you want LaTeX's default font
\usepackage{url}      % llt: nicely formatted URLs
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{booktabs, multicol, multirow}
\usepackage{graphicx}
\usepackage{subfigure}
% llt: Define a global style for URLs, rather that the default one
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\bf\ttfamily}}}
\makeatother
\urlstyle{leo}


% To make various LaTeX processors do the right thing with page size.
\def\pprw{8.5in}
\def\pprh{11in}
\special{papersize=\pprw,\pprh}
\setlength{\paperwidth}{\pprw}
\setlength{\paperheight}{\pprh}
\setlength{\pdfpagewidth}{\pprw}
\setlength{\pdfpageheight}{\pprh}

% Make sure hyperref comes last of your loaded packages,
% to give it a fighting chance of not being over-written,
% since its job is to redefine many LaTeX commands.
\usepackage[pdftex]{hyperref}
\hypersetup{
pdftitle={SIGCHI Conference Proceedings Format},
pdfauthor={LaTeX},
pdfkeywords={SIGCHI, proceedings, archival format},
bookmarksnumbered,
pdfstartview={FitH},
colorlinks,
citecolor=black,
filecolor=black,
linkcolor=black,
urlcolor=black,
breaklinks=true,
}

% create a shortcut to typeset table headings
\newcommand\tabhead[1]{\small\textbf{#1}}


% End of preamble. Here it comes the document.
\begin{document}

\title{Mining Frequent ADL Patterns via Acceleration Data}

%\numberofauthors{2}
%\author{
%  \alignauthor Zehong Tan\\
%    \affaddr{School of Software Engineering}\\
%    \affaddr{Shanghai Jiao Tong University, Shanghai}\\
%    \email{tanzehong@aliyun.com}
%  \alignauthor Shuangjiu Xiao\\
%    \affaddr{School of Software Engineering}\\
%    \affaddr{Shanghai Jiao Tong University, Shanghai}\\
%    \email{xsjiu99@cs.sjtu.edu.cn}
%}

% Teaser figure can go here
%\teaser{
%  \centering
%  \includegraphics{Figure1}
%  \caption{Teaser Image}
%  \label{fig:teaser}
%}

\maketitle

\begin{abstract}
Physical activity recognition is an active area in sensor mining and ubiquitous computing community. 
One challenge is to handle large amount of data which does not belongs to pre-defined classes.
In this paper, we propose an unsupervised method to discover frequent Activity of Daily Living (ADL) patterns from acceleration data streams.
Once an frequently-occurred ADL pattern is discovered, our approach learns a classifier for it.
And the approach can keep digging for new classes of activity when new acceleration stream comes.
Our approach can be divided into two phrases:
 A rough segmentation (pattern discovery) of the motion stream via topic model, and model learning with semi-supervised learning, whose training set is obtained by segmentation results from the first stage.
 Experiments on real life datasets show promising results on both ability to discover ADL patterns and recognizing accuracy of its learned classifiers.
\end{abstract}

\keywords{
	ADL discovery; ADL recognition; Pattern discovery
}

\section{Introduction}

Activities of Daily Living (ADL) recognition is an active research area in senor mining and wearable computing.
Unlike ``simple activity" such as ``walking, sitting, running" and some given gestures, an ADL level activity such as ``having dinner", ``vacuuming" is composed of a series of sub-activities in a non-stable order.
The recognition of ADLs such as ``having dinner", ``working at desk", ``dressing up" provides context-based applications with a higher level abstraction of the user context.
With the popularization of smart phones and wearable gadgets such as smart bracelets, the efficient discovery and recognition of ADL is able to provide useful context to applications ranging from healthy care to context-aware smartphone, from education and smart home.

The generally accepted scheme to recognize activities is to use machine learning techniques to map pre-segmented acceleration data to the corresponding pre-defined activity labels. 
However, the online recognition and personalized activity modeling introduces challenges that do not occur in the generally accepted offline approach.
One of the challenges is that the user's actual activities may not occur in the pre-defined label list. It requires the system to automatically discover these out-of-vocabulary activity patterns, especially when the out-of-vocabulary data occupies the majority of the observed data.

In this paper, we propose an unsupervised approach to discover ADL patterns from unlabeled acceleration data.
We take unlabeled acceleration stream as input and learned classifiers (one for each discovered ADL pattern) as output. Our method is based on topic model and semi-supervised learning.

By discovering frequent ADL patterns unsupervisedly, our approach adapts itself to the user's out-of-vocabulary activity patterns. Also it offers the opportunity to build a personalized model by leveraging the user's own motion data, in a way where the class of learned model and the parameters of learned model is adaptive the user's own daily routine and motion style.

The main contributions of this paper is as follows.
First, it propose an unsupervised scheme to discover ADLs in acceleration stream and learned classifiers for each discovered pattern.
Secondly, it applies LDA model directly on acceleration data to perform an ADL-grained segmentation.

The rest of the paper is organized as follows.
Section ``Approach Overview" gives an overview of our design.
Section ``Raw Segmentation with LDA" discusses how to segment the acceleration stream with LDA model. Section ``Learning a Classifier" introduces the ADL learning process based on the earlier segmentation. Experiments are performed in Section ``Experiments".
Finally, in Section ``Conclusion and Future Work" we summarize our results and give an outlook on future work.


\section{Related Work}
\label{sec.related-work}

Earlier human activity recognition focus on gestures and simple activities such as walking, running, going upstairs \cite{kwapisz2011activity, liang1998real, mantyjarvi2001recognizing}.
In 2012,Rai et al \cite{rai2012mining} recognized complex activities from acceleration data in smartphone. Their approach recognizes complex activities in a two stage scheme on data collected from smartphone over 8 weeks, and reached impressive results.

A couple of researches ease the labeling efforts by leverage techniques of semi-supervised learning and multi-instance learning.
Maja et al \cite{stikic2008exploring} explored the performance of co-training and active learning in ADL recognition, and found that the train data can be reduced to a surprisingly small amount.
Stikic et al \cite{stikic2009activity} applied multi-instance learning to provides user with more abstract annotation ways.

Another line of research avoids the labeling efforts by unsupervised discovery.
ADR-SPLDA \cite{chikhaoui2012adr} explored the activity discovery in the smart home with extensive sensors in a hierarchical approach. Cook et al\cite{cook2013activity} also discovered activity pattern in an smart environment by discovering frequently-occurred event sequences.
Huynh et al \cite{huynh2006unsupervised} and Minnen et al \cite{minnen2006discovering} detected certain simple activities from on-body acceleration data in an unsupervised way. Topic model is a popular method in unsupervised activity discovery.
Typical ways to use topic model in activity discovery is a hierarchical approach.
First, researchers learn low-level short activity or posture in an supervised way and convert the raw sensor stream into low-level activity sequences.
Then topic model is applied on these activity sequences to discovered the high-level pattern.
Huynh \cite{huynh2008discovery} applied topic model on low-level recognized activities to discovery daily routine pattern.
ADR-SPLDA \cite{chikhaoui2012adr} also applied topic model on low-level recognized sequences.
Pathak et al \cite{wang2009human} uses the similar approach in video stream mining.


\section{Approach Overview}
\label{sec.overview}

    \subsection{Design Concerns}
    The goal of our system is to mine ADL activities from hours of on-body sensor stream unsupervisedly.
    The system discovers frequently-occurred ADL pattern from acceleration stream and learn a classier for each discovered pattern.

    We'd like our system to discover human activities in such a manner:
    \begin{itemize}
        \item \textbf{Online discovering}
        The system should be able to discover activities continuously from continuous sensor data.
        It differs from the offline approach, which trains the activity classifiers from a fixed training set.
        Our system should keep mining when new sensor data comes.
        It should recognize the known activity classes with its known classifiers and discover new activity patterns from the new data.

        \item \textbf{Scalable to various activity class}
        The number of activity class won't be manually configured in advance.
        The system should decide itself how many activities to model.

        \item \textbf{Personalization}
        The label of classifiers could be personalized to the user's daily routine and the parameters of each classifier could be personalized to his own motion style.
    \end{itemize}

\subsection{Framework of Our Approach}

    We design our system as a unsupervised learning system to meet these requirements.
    Since our method is unsupervised, we may only use unlabeled data, thus the learning process could keep on discovering once after new unlabeled data collected.
    Also, we may able to leverage the user's own unlabeled data.
    Therefore the classifiers learned could be personalized.

    Figure \ref{fig: flowchart} illustrates the overall framework of our design.
    Our method could be divided into two phrases.
    The first phrase applies an LDA algorithm to do a rough segmentation to the raw acceleration stream.
    We apply the LDA estimation to documents generated over a slide window of 30 seconds and shifted by 30 seconds.
    Then the raw segmentation is performed by analyzing the distribution of topics of the neighboring windows.
    The second phrase generates training examples for a certain activity from the segmentation result and learns the classifier for this activity with semi-supervised learning techniques.

    \begin{figure}
     \includegraphics[height=3in]{MotionDiscoveryFlowChart.jpg}
        \caption{Framework of mining ADLs from acceleration data with LDA and semi-unsupervised learning }
        \label{fig: flowchart}
    \end{figure}

    The activities in the dataset are usually heavily unbalanced.
    Some activities (e.g. ``sitting desk activities", ``sleeping deeply") appears more times and lasts much longer than other activities (e.g. ``fanning at barbecue").
    We tackle this problem by doing the mining process iteratively.
    After the recognized data with learned classifiers are more than a threshold, we will filter out these recognized data and start the whole learning process all over again -- from raw segmentation to classifier learning.

    This scheme can adapt itself to new motion data in an intuitive way.
    When the new data comes, the earlier learned classifiers first filter out these known patterns and left the unknown ones.
    Then the remaining samples are reorganized in time-increasing order and go through the mining process like a normal stream.
    The recognized samples can serve as negative samples for classifier training.
    In this way, the scheme can keep mining new patterns from new data stream once after a couple of hours.

\section{Raw Segmentation with LDA}
 \label{sec.segmentation}

 One tough problem in our unsupervised scheme is how to leverage all these unlabeled and even unsegmented data.
 In the experiment, we found that when we apply topic model on windows of raw data, if the neighboring windows belongs to the same ADL, then the topic distribution of these windows tends to be similar (Figure.\ref{fig: tm-intuition} (a) ``having dinner", (b) ``queue in line").
 However, if the neighboring windows are noise data or transition data between two activities, then the topic distributions tends to be shapeless (Figure.\ref{fig: tm-intuition} (c) ``noise").

\begin{figure}
    \subfigure[]{
  \includegraphics[height=1.5in]{tm-having-dinner.jpg}}
  \subfigure[]{
  \includegraphics[height=1.5in]{tm-queue-in-line.jpg}}
  \subfigure[]{
  \includegraphics[height=1.5in]{tm-noise.jpg}}
    \caption{Topic distribution of motion windows when applying LDA to motion streams. One color represents one topic. (a) having dinner (b) queue in line (c) noise data }
    \label{fig: tm-intuition}
\end{figure}

 Inspired by this fact, we sliced the raw acceleration data into segments by analyzing the topic distribution of neighboring windows.
 Windows in one segment are regarded as data of the same activity class, then can be treated as the positive examples for this class of activity.

 In this section, we will first introduce briefly about LDA model, and then our ways to apply this method to raw acceleration data, and finally introduce our rough segmentation approach based on the result of LDA.

\subsection{Topic Model with LDA}

    Topic model \cite{randell2000context} stems from techniques in natural language processing.
    It regards a piece of document as a collection of words, regardless of the word's position.
    Each word is also related to one or some topics, thus a piece of document could be regarded as a combination of some topics.
    For example, a document on ``The Human Genome Project" is regarded as a collection of words sampling from topic ``Human Evolution", ``Genetics" and ``Computer Science".

    Topic model can be used to generate a document automatically.
    More interestingly, topic model could infer the topic distribution of each document and the word distribution of each topic, given a collection of documents.
    The Latent Dirichlet Allocation (LDA), as a statistical model, is usually used to capture this intuition.
    To understand how LDA works, let's first describe the generative process -- how to generate a document given a distribution of topics.

    Topic $z$ is defined as a distribution over a fixed vocabulary.
    A document $d$ is regards as a distribution over a set of topics $z_1,z_2,\ldots,z_T$.
    To generate a word $w$ in a document at a certain place, we first randomly choose a topic over the topic distribution, and then random choose a word in the chosen topic over the word distribution.
    This process could be modeled in the formula below.
        \begin{equation} p(w|d) = \sum_{z=1}^T p(w|z)p(z|d)\end{equation}
    LDA models the reversed process.
    The documents are observed, while the topic distribution over documents and word distribution over topic are yet to be inferred.
    LDA treats the observed documents as arising from the generative process that includes hidden variables \cite{randell2000context}, which is the hidden topic structure.
    LDA computes the conditional distribution of the hidden variables given the observed variables by using joint distribution.
    This problem could be reduced to find parameters $\alpha$ for the dirichlet distribution and parameters $\beta$ for the topic-word distributions $p(w|z,\beta)$ that maximize the likelihood L of the data for documents \cite{huynh2008discovery} d = 1, ...M:
        \begin{equation} L(\alpha, \beta) = \prod_{d=1}^M \int p(\theta_d | \alpha)(\prod_{n=1}^{N_d}\sum_{z=1}^T p(w_n^d | z, \beta) p(z|\theta_d)) d\theta_d \end{equation}
        where $\theta_d$ is the topic distribution over document $d$ and each document $d$ consists of words $w_n^d (n=1,2,\ldots,N)$.

    LDA assumes that the order of words does not matter.
    This assumption makes it suitable for exploring ADL activity structure.
    Because ADLs are often composed of a series of sub-motions and gestures, and they are often performed as interleaved and in an random order.

    \subsection{Generating Motion-Document}

    In this section, we will show how LDA is applied on acceleration data streams by introducing motion-documents.
    A motion-document is defined as a feature collection over a sliding window of 30 seconds shifted in increments of 30 seconds.
    Thus, raw acceleration stream over hours could be regarded as a collection of documents.
    Each document contains the motion information about a 30-second-long activity.

    As words to a plain document, a frame-based feature is a word in our motion-document.
    The dataset is sub-sampled at a relatively low frequency at 2Hz, so we have 60 frames in a motion-document.
    To each frame, these features are extracted on each sensor channel: mean and standard deviation,which sums up into F features.
    Since these features are real-number features, we can not use them as a word. This problem is tackled by binning the feature value into $N_{bin}$ equal-frequency bins.
    Thus, the vocabulary size is $N_{bin} * F$.

    To sum up, a motion-document, representing 30-second-long motion data, has $60F$ word with 60 frames. And $N_{bin} * F$ different words compose our vocabulary.

    \subsection{Segmentation of Acceleration Stream based on LDA}

    As the result of document generating procedure, we obtain a list of motion-documents representing hours of human motions.
    Our goal is to give a rough segmentation of these motion data, where continuous data of the same activity class is segmented into one segment.
    We achieve this by applying LDA model on motion-documents and segmenting based on their topic distributions.

    Figure \ref{fig: day1-tm_sample} shows the result of LDA model on motion-documents of two fragments in Ubicomp Day-1 data with K (K=10) topics.
    A point on x-axis represents a motion-document, listed in time-increasing order.
    y-axis represents the topic distribution over one motion-document. One line of the same color represents one topic.
    The topic distribution of motion document $d$ is defined as $TD(d)$. $TD(d, k)$ refers to the $k$th topic's proportion over document $d$.
    For example in Figure \ref{fig: day1-tm_sample} (b), motion-documents of activity ``discussing at whiteboard" are almost composed by vocabulary in topic-orange, while motion documents of activity ``sitting desk working" are a mixture of topic-blue, topic-green and topic-pink.

     \begin{figure}
         \centering
         \subfigure[]{
            \includegraphics[width=3.4in, height=1.3in]{seg1_tagged.jpg}
         }
         \vspace{0.1cm}
        \subfigure[]{
            \includegraphics[width=3.4in, height=1.3in]{seg2_tagged.jpg}
        }
        \caption{Topic distribution of motion-documents in Day-1}
        \label{fig: day1-tm_sample}
    \end{figure}

     By observing Figure \ref{fig: day1-tm_sample}, we can find that usually topic distribution of the same activity class over a period of time tends to be similar, while the topic distribution in activity transition time tends to be noisy and shapeless.
     Inspired by this observation, we segment the motion-document list in the following way:

    1. If neighboring $n$ ($n=5$) motion-documents share similar document distribution, then these $n$ motion-documents are regarded as a mini-segment.
    This is done computing the similarity of topic distribution $SimTD$ over a sliding window of size $n$ and shifted by 1.
        \begin{equation}
            \begin{split}
             SimTD_{t,n} = -\sum_{k=1}^K sd(TD(d_t, k), TD(d_{t+1}, k),  \\ TD(d_{t+2}, k), \ldots,
              TD(d_{t+n-1}, k))
            \end{split}
        \end{equation}

        ,where $sd()$ refers to standard deviation.

    2. Merge the neighboring mini-segments together.

    The proposed segmentation approach is a rough estimation of activity segmentation.
    Figure \ref{fig: day1-segmentation-result} illustrate the segmentation results of Day-1 data.
    Ideally, documents with the same activity class over a relatively long period of time are estimated into one segmentation (e.g. ``discussing at whiteboard", ``lying using computer").
    However, some long activities are cut into several segments (e.g. ``sitting desk working").
    This may due to some reasons.
    (1) This activity are somehow interrupted by some short activities (e.g.``stand up and go for a drink"). (2) The activity is so long that makes the document set unbalanced and force the LDA to model it on a more fine-level granular.
    (3) The user may changes his posture style slightly during the long activity.
    This problem can be eased by semi-supervised learning in the next phrase.

    \begin{figure}
        \centering
        \includegraphics[width=3.4in, height=0.85in]{segmentation_day1.jpg}
        \caption{Segmentation result of acceleration data stream in Day-1}
        \label{fig: day1-segmentation-result}
    \end{figure}


\section{Learning a Classifier}
\label{sec.learn-a-classifier}

    \subsection{Training Samples}
    Now that the motion stream is already cut into segments, our goal is to learn the classifiers for ADL classes.
    One main decision to make is how to generate the positive and negative training samples for learning.
    Here, we propose a method to automatically generate training examples.
    First we find the longest un-recognized segmentation, and regard it as our target.
    Our goal is to find a classifier that can recognize all the motion samples that has the same ADL class with the samples in this target segment.
    Motion samples are also obtained by a slide window over 30 seconds and shifted by 30 seconds, one to one correspondent to a motion-document.
    Features of sample are the mean, standard deviation and entropy, energy on frequency space \cite{rai2012mining} over each channel.
    Ideally the classifier will not only recognized the samples in the target segment correctly,
    it will also identify other segments which are filled with samples with the same activity class.

    Positive samples can be easily gained.
    We randomly sample 70\% samples from the target segment as the positive training examples.

    Negative samples are more complex.
    Balance between the aggressive and conservative need to be considered.
    An aggressive sampling approach may take in extensive negative samples and even true samples, which brings the classifier with high specificity but low sensitivity.
    On the other hand, the conservative approach makes sure that the negative examples we get are true negative ones, which leads to high sensitivity but low specificity due to insufficient negative examples.
    To achieve the balance, we propose a method that sampling negative samples by considering the difference of topic distribution between two segments.

    Difference of topic distribution between segment $S_1(d_{t_1}, d_{t_1+1}, \ldots, d_{t_1 + n_1 - 1})$ and $S_2(d_{t_2}, d_{t_2+1}, \ldots, d_{t_2 + n_2 - 1})$ measures the difference between two segments.
    It is measured by the mean proportion of each topic in the segment, which is defined as follows:
        \begin{equation}
            \begin{split}
              Diff(S_1, S_2) =  \sum_{k=1}^K \| \frac{1}{n_1}\sum_{j=0}^{n_1-1} TD(d_{t_1+j}, k) \\
               - \frac{1}{n_2}\sum_{j=0}^{n_2-1} TD(d_{t_2+j}, k)\|
            \end{split}
        \end{equation}
    The negative training samples for target segment $S_{target}$ is obtained in the following way:
        \begin{itemize}
        \item \textbf{short segment $S$}: treated as noise

        \item \textbf{long similar segment $S$} ($Diff(S, S_{target}) < threshold$):
        The sample $S$ is hard to tell whether belongs to the target activity. We treat it as uncertain samples.

        \item \textbf{long different segment $S$} ($Diff(S, S_{target}) > threshold$):
        The sample $S$ behaves quite different to the target segment and could be used as negative samples. To take full advantage of its difference, we sample the negative ones proportional to $Diff(S, S_{target})$.
        \end{itemize}
    \subsection{Learn a Classifier with Semi-Supervised Learning}

    Since our training examples are obtained from segmentation estimation rather than manually picked, our training examples are very limited in both number and variety.
    One of the best choices in learning a classifier with these limited data is to learn it with semi-supervised learning.
    Research on semi-supervised learning on activity recognition \cite{stikic2008exploring} indicates that a classifier trained with only 6.3\% data by co-training performs surprisingly competitive to the one with all data.
    Here, we learn the classifier for the target segment with co-training technique.

    Co-training is a wrapper-algorithm that repeatedly improve self by augmenting the training process with an additional source of information \cite{stikic2008exploring}.
    It is ideally trained with two views of the same data.
    If we had the GPS data or audio data of the same user in the same day, that would do us the best.
    Unfortunately, we don't.
    An inferior choice for us is to use two different classification algorithm.
    Here we use decision tree and SVM.
    Decision tree and SVM train two classifiers separately on the training data that we gained from segmentation, and teach another one by augmenting each other's training set with its own most confident predications on the remaining samples.
    In our experiment, we repeated the training and teaching iteration by four times and use the SVM classifier on the fourth iteration as the final classifier.

    Figure \ref{fig: day1-result} shows the labeling results of three learned classifiers on data Ubicomp Dataset Day-1.
    Comparing the classes labeled by the classifiers with the ground truth, we find that our labeling result has high correlation to the ground truth, which indicates that our approach can successfully discover patterns in an ADL's granulate.
    Due to the high correlation, we regard these three classifiers as the model of activity ``sitting desk working", ``lying using computer" and ``discussing at whiteboard".
    These three activities covers 81\% samples in the day-1 dataset.
    And the learned classifiers achieve accuracy of 71\%, 80\% and 95\%.

\begin{figure}
    \centering
  \includegraphics[width=3.4in, height=0.9in]{day1-desk-lyingcomputer-discussing-dot-legend.jpg}
    \caption{Three activities discovered in Day-1}
    \label{fig: day1-result}
\end{figure}


\section{Experiments}
\label{sec.experiments}

    While Figure \ref{fig: day1-result} suggests that our approach are able to discover ADL patterns from unlabeled acceleration streams, it is not obvious how to quantify the results.
    We propose three metrics in this section to measure its ability on discovery ADL patterns and accuracy of its classifiers.
    Section ``ADL Discovery" and ``New Patterns Discovery on New Data" focus on examine the ability on discovering patterns from acceleration streams.
    Section ``Recognize ADLs" focuses on measuring the recognition accuracy of the learned classifiers.

    \subsection{Datasets}
    
    \subsubsection{Ubicomp Dataset}
    Ubicomp08 dataset \cite{huynh2008discovery} recorded the daily life of one person over a period of sixteen days, of which seven days are annotated with ADL level.
    Two wearable accelerometers were placed at the dominant wrist and in the right pocket (i.e. slightly below the right hip) of a single male subject as he was performing everyday activities .
    The accelerometers collects data at 2Hz frequency.
    Each day of the records consists of roughly 12 hours of data.
    
    \subsubsection{PAMAP Dataset}
    PAMAP dataset \cite{reiss2012introducing, reiss2012creating} contains data 12 different activities (including lying, walking, vacuum cleaning, ironing, etc), performed by 9 subjects wearing three inertial sensors and a heart rate monitor.
    The three inertial units are worn at chest, hand and ankle, recording 3D acceleration data at 100Hz.
    The data is collected in realistic scenes.
    Altogether, over 10 hours of data is collected, of which nearly 8 hours are annotated.
    

    \subsection{ADL Discovery}
    \label{subsec.exp.mining-on-same-day}

    Since our approach discovers ADLs patterns in an unsupervised way, one of the common scenes to use our approach is make it to mine and label hours of sensor data without any prior knowledge.
    Here, we applied our method on one-day-long sensor data, and evaluated its performance on its discovering coverage and accuracy.
    Namely, we focus on
        \begin{itemize}
        \item \textbf{Activity count} How many ADL patterns are discovered?
        \item \textbf{Discovery coverage} How many samples in the dataset can be covered by the recognized pattern classes?
        \item \textbf{Recognition accuracy} How many samples in the dataset are correctedly labeled?
        \end{itemize}
    One thing to mention is that in this section's experiment, the dataset that we test on is the same dataset that we applied our mining algorithm on.

     We applied our our mining algorithm on each day of Ubicomp dataset.
     Ten activity patterns (Table \ref{tab: ADL patterns discovered}) are discovered in seven days, includes ``sitting-desk-working", ``discussing-at-whiteboard", ``driving-bike", ``fanning-in-barbecue", etc.
     Table \ref{tab:result-seven-days} shows the mining results of seven days.
     In this table, The ADL pattern, coverage, recognizing sensitivity and accuracy are listed.
     The average coverage of discovered patterns reaches 86\%.
     The result also shows the good adaptivity of our scheme to new activity patterns.
     Day-4 seems a outing barbecue day in the dataset. Some barbecue related activities appear only on that day.
     The approach adapts to this phenomenon by successfully discovering pattern "kneeling-making-fire" and "fanning at barbecue".

     The overall labeling accuracy reaches 75\% sensitivity and 79\% accuracy.
     The accuracy is relatively better for activities that performed in a continuous period of time, e.g. ``watching-movie" (Day-5), ``discussing-at-whiteboard"(Day-1, Day-4).
     Activity ``sitting-desk-working" obtains a relatively lower labeling accuracy.
     One reason for that may be that, the person performs this activity during most of the day, but his working style may differ from morning to afternoon.
     Some classifiers detects his morning desk working sensitively but ignores his afternoon desk working.
     This phenomenon occurs on Day-6. Three pattern are discovered, namely corresponding to ``sitting-desk-working" at in the morning, in the afternoon and in the evening.
     Another reason is that although desk working occupies most of the time, it is interrupted by many other activities.
     Therefore the segment for desk working is not very long and we don't have sufficient positive examples for training.

    % Table generated by Excel2LaTeX from sheet 'Sheet6'
\begin{table}[htbp]
    \begin{tabular}{rr}
    \toprule
    \multicolumn{2}{c}{\textbf{Discovered ADL Patterns}} \\
    \midrule
    \multicolumn{1}{l}{Sitting desk working} & \multicolumn{1}{l}{Lying using computer} \\
    \multicolumn{1}{l}{Discussing at whiteboard} & \multicolumn{1}{l}{Driving bike} \\
    \multicolumn{1}{l}{Driving car} & \multicolumn{1}{l}{Having a meal} \\
    \multicolumn{1}{l}{kneeling making fire} & \multicolumn{1}{l}{Attending a meeting} \\
    \multicolumn{1}{l}{Fanning at barbecue} & \multicolumn{1}{l}{Watching movie} \\
    \bottomrule
    \end{tabular}%
    \caption{ADL patterns discovered in Ubicomp dataset}
  \label{tab: ADL patterns discovered}%
\end{table}%

% Table generated by Excel2LaTeX from sheet 'Sheet2'
\begin{table}[htbp]
  \centering
    \setlength{\tabcolsep}{2pt}
    \def\arraystretch{1.2}
    \begin{tabular}{llrrr}
      \toprule
          &       & \textbf{Coverage} & \textbf{Sensitivity} & \textbf{Accuracy} \\
    \midrule
    \multirow{4}[0]{*}{\textbf{D1}} & Sitting desk working & 0.69  & 0.68  & 0.71  \\
          & Lying using computer & 0.08  & 1.00  & 0.80  \\
          & Discussing at whiteboard & 0.04  & 1.00  & 0.95  \\
          & \textbf{Average} & \textbf{0.81 } & \textbf{0.89 } & \textbf{0.82 } \\
          \hline
    \multirow{4}[0]{*}{\textbf{D2}} & Sitting desk working & 0.75  & 0.67  & 0.70  \\
          & Attending a meeting & 0.08  & 0.25  & 0.92  \\
          & Driving bike & 0.04  & 1.00  & 0.94  \\
          & \textbf{Average} & \textbf{0.87 } & \textbf{0.64 } & \textbf{0.85 } \\
          \hline
    \multirow{3}[0]{*}{\textbf{D3}} & Sitting desk working & 0.79  & 0.63  & 0.67  \\
          & Having a meal & 0.05  & 0.49  & 0.72  \\
          & \textbf{Average} & \textbf{0.84 } & \textbf{0.56 } & \textbf{0.70 } \\
          \hline
    \multirow{6}[0]{*}{\textbf{D4}} & Sitting desk working & 0.66  & 0.63  & 0.74  \\
          & Discussing at whiteboard & 0.07  & 0.99  & 0.73  \\
          & Kneeling making fire & 0.02  & 0.95  & 0.95  \\
          & Walking & 0.05  & 0.38  & 0.92  \\
          & Fanning at barbecue & 0.03  & 0.98  & 0.83  \\
          & \textbf{Average} & \textbf{0.83 } & \textbf{0.79 } & \textbf{0.83 } \\
          \hline
    \multirow{4}[0]{*}{\textbf{D5}} & Sitting desk working-1,2 & 0.84  & 0.84  & 0.57  \\
          & Watching movie & 0.08  & 1.00  & 0.75  \\
          & Having a meal & 0.05  & 0.72  & 0.85  \\
          & \textbf{Average} & \textbf{0.87 } & \textbf{0.85 } & \textbf{0.72 } \\
          \hline
    \multirow{3}[0]{*}{\textbf{D6}} & Sitting desk working-1,2,3 & 0.83  & 0.97  & 0.93  \\
          & Driving car & 0.06  & 0.67  & 0.86  \\
          & \textbf{Average} & \textbf{0.89 } & \textbf{0.82 } & \textbf{0.90 } \\
          \hline
    \multirow{3}[0]{*}{\textbf{D7}} & Lying using computer & 0.23  & 0.95  & 0.77  \\
          & Sitting desk working & 0.64  & 0.39  & 0.60  \\
          & \textbf{Average} & \textbf{0.87 } & \textbf{0.67 } & \textbf{0.69 } \\
          \hline
    \textbf{} & \textbf{Summary} & \textbf{0.86 } & \textbf{0.75 } & \textbf{0.79 } \\
    \bottomrule
    \end{tabular}%
  \caption{Discovered ADL patterns in seven days}
  \label{tab:result-seven-days}%
\end{table}%


\subsection{New Patterns Discovery on New Data}
    \label{subsec.exp.discover-new-classes}

    Our system is an online learning system, it can keep taking in new data and exploring new activity classes once after a while.
    When new sensor data comes, the learned classifiers will first recognize samples in the new dataset and attach labels for them, and then we will keep on mining the remaining data for new activity classes.
    The remaining data makes up the new dataset, and go through stages of segmentation and learning.
    Here, we test our algorithm of the ability of discovering new activity classes with new data, with some old learned classifiers.
    We take the classifiers learned in day-1 as known models, and day-2 and day-4 data as new data.
    After recognizing known three activities, we also found new patterns corresponding to ``having a meal" ``driving bike" in day-2 and ``kneeling making fire" ``fanning barbecue" in day-4.

    \subsection{Recognize ADLs}
    \label{subsec.exp.recognize-performance}

    We also explored the recognizing performance of our learned classifiers.
    We learned the models based on one day data and test them on other six days.
    Table 3 shows the recognizing accuracy of some frequently occurred activities.
    The overall recognizing performance reaches 69\% sensitivity and 78\% accuracy.

    % Table generated by Excel2LaTeX from sheet 'Sheet4'
    \begin{table}[htbp]

    \begin{tabular}{rrr}
    \toprule
          & \textbf{Sensitivity} & \textbf{Accuracy} \\
    \midrule
    \multicolumn{1}{l}{Sitting desk working} & 0.64  & 0.67  \\
    \multicolumn{1}{l}{Discussing at whiteboard} & 0.97  & 0.76  \\
    \multicolumn{1}{l}{Having a meal} & 0.46  & 0.70  \\
    \multicolumn{1}{l}{Driving car} & 0.92  & 0.86  \\
    \multicolumn{1}{l}{Walking} & 0.45  & 0.93  \\
    \multicolumn{1}{l}{\textbf{Average}} & \textbf{0.69}  & \textbf{0.78}  \\
    \bottomrule
    \end{tabular}%
  \label{tab:model-accuracy}%
  \caption{Recognizing performance of learned classifiers}
\end{table}%

\section{Conclusion and Future Work}
\label{sec.conclusion}

    In this paper, an unsupervised approach is proposed to discover and recognize ADLs from acceleration streams from wearable devices.
    It tackles the weakness of no labeled training data by obtaining positive/negative training data from LDA-based segmentation.
    The LDA segmentation not only cuts the raw stream into segments roughly, it also measures the similarity between segments based on the distribution of topics.
    Experiments on Ubicomp dataset indicates the promising discovering ability and recognizing accuracy of our scheme.
    The experiment was done on a daily bases. Ten ADL patterns are discovered and coverers 86\% of the data.
    The learned classifiers reaches 75\% sensitivity and 79\% accuracy on the same day and 69\% sensitivity and 78\% accuracy on other days.

     However, our mining approach is unable to discover patterns for short activities such as ``washing hands" due to its segmentation policy.
     These short activity usually lasts for 2-3 motion-documents long and thus will be treated as noise.
     Another shortcoming is that we only mines the data on a daily bases once a time.
     A larger dataset (e.g. multiple days) are not helping due to two reasons.
     First, the computer has insufficient memory to model a larger dataset with LDA.
     Second, since we obtain the positive training data from one long segment, multiple days of data won't make the segments longer and thus won't enlarge variety of positive training data.

    In the future, we plan to enhance mining scheme in the following manner.
    First, we plan to enhance the learned classifier iteratively in its online mining scheme.
    When new unlabeled data comes, the learned classifier may able to leverage the new data and enhance its recognizing sensitivity.
    Second, we plan to offer the user an intervention scheme by offering a coarse-grain annotation interface.



% Balancing columns in a ref list is a bit of a pain because you
% either use a hack like flushend or balance, or manually insert
% a column break.  http://www.tex.ac.uk/cgi-bin/texfaq2html?label=balance
% multicols doesn't work because we're already in two-column mode,
% and flushend isn't awesome, so I choose balance.  See this
% for more info: http://cs.brown.edu/system/software/latex/doc/balance.pdf
%
% Note that in a perfect world balance wants to be in the first
% column of the last page.
%
% If balance doesn't work for you, you can remove that and
% hard-code a column break into the bbl file right before you
% submit:
%
% http://stackoverflow.com/questions/2149854/how-to-manually-equalize-columns-
% in-an-ieee-paper-if-using-bibtex
%
% Or, just remove \balance and give up on balancing the last page.
%
\balance

% If you want to use smaller typesetting for the reference list,
% uncomment the following line:
% \small
\bibliographystyle{acm-sigchi}
\bibliography{sample}
\end{document}
